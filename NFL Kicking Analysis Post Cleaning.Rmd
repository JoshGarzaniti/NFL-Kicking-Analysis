---
title: "NFL Kicking Analysis (Post-Cleaning)"
author: "Josh Garzaniti"
date: "2025-07-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Packages I used:
```{r}
library(ggplot2)
library(ggdark)
library(caret)
library(tidyverse)
library(tidyr)
library(stringr)
library(xgboost)
library(randomForest)
library(ggrepel)
library(nflreadr)
library(nflplotR)
library(nflfastR)
library(mice)
library(lme4)
library(arm)
library(car)
library(effects)
library(Matrix)
library(pROC)
library(SHAPforxgboost)
library(ROSE)
library(Metrics)
library(shiny)
library(DT)
```


## Analysis
```{r}
pbp_complete = read.csv("G:/My Drive/Personal Projects/pbp_complete.csv")
```


Now we don't have to deal with missing data anymore. The next step is creating target variables. We already created on called "result" which in essence was just a make or miss variable based on if a kicker made their try attempt, but now let's make a derived variable based off of that value called probability, which is the probability of that kicker making that attempt.

P(x) we can determine a few ways:

we can group all of the attempts by distance, distance and weather, distance, weather turf condition, distance, weather, turf condition, temperature, and wind......or all of those and throw in the stadium. In that sense what we are doing is taking a historical average of how many attempts from that spot on that field in those conditions were made vs missed.....the thing we have to be smart about is not narrowing our sample size down too much to where we condense the number of tries going into the historical average. E.g say a there's only been one attempt at Lambeau field from 65 yards out, and miraculously the kicker made it.....on the next attempt from that location a kicker would be going up against a p(x) of 100% (which in theory says they should make that kick 100% of the time), yet we all know that's not factual in reality.

if we group up by:
distance
roof type
temperature
wind speed
and surface type
let's see what the empirical results of each attempt are:

```{r}
pbp_complete_grouped = pbp_complete%>%
  group_by(kick_distance, roof, temp, wind, surface)%>%
  mutate(probability_advanced = mean(result, na.rm = TRUE))%>%
  ungroup()

pbp_complete_grouped
```

A good baseline for this is to just check by distances (our probability should be dropping of drastically by distance so let's see what that distribution looks like)

```{r}
distance_vs_probability_plot = ggplot(pbp_complete_grouped, 
                                      mapping = aes(x = kick_distance,
                                                    y = probability_advanced))+
                                      geom_point()+
                                      geom_smooth()+
                                      dark_theme_minimal()+
                                      labs(title = "NFL Kick Distance vs Make Probability (advanced)", subtitle = "from 1999-2024", x = "Distance", y = "Probability")

distance_vs_probability_plot
```
What about binning vars like temp and wind?

```{r}
pbp_complete_binned = pbp_complete%>%
  mutate(
    temp_bin = cut(temp, breaks = c(-Inf, 30, 50, 70, 90, Inf), labels = c("cold", "chilly", "mild", "warm", "hot")),
    wind_bin = cut(wind, breaks = c(-Inf, 5, 10, 20, Inf), labels = c("calm", "breezy", "windy", "gusty")))%>%
  group_by(kick_distance, roof, temp_bin, wind_bin, surface)%>%
  mutate(
    group_n = n(),
    probability_advanced = ifelse(group_n >= 5, mean(result, na.rm = TRUE), NA_real_))%>%
  ungroup()%>%
  mutate(
    probability_advanced = ifelse(is.na(probability_advanced), mean(result, na.rm = TRUE), probability_advanced))
```

Visualizing this

```{r}
distance_vs_probability_plot_2 = ggplot(pbp_complete_binned, 
  aes(x = kick_distance, y = probability_advanced)) +
  geom_point(aes(size = group_n), alpha = 0.6) +
  geom_smooth() +
  geom_hline(
    yintercept = mean(pbp_complete$result, na.rm = TRUE), 
    linetype = "dashed", 
    color = "red", 
    alpha = 0.6)+
  dark_theme_minimal()+
  labs(
    title = "NFL Kick Distance vs Make Probability (advanced)", 
    subtitle = "from 1999-2024", 
    x = "Distance", 
    y = "Probability")

distance_vs_probability_plot_2
```

Facet Wrap by Roof type

```{r}
distance_vs_probability_roof = ggplot(pbp_complete_binned, 
  aes(x = kick_distance, y = probability_advanced))+
  geom_point(aes(size = group_n), alpha = 0.6)+
  geom_smooth(se = FALSE)+
  geom_hline(
    yintercept = mean(pbp_complete$result, na.rm = TRUE), 
    linetype = "dashed", color = "red", alpha = 0.6)+
  facet_wrap(~ roof)+
  dark_theme_minimal()+
  labs(
    title = "NFL Kick Distance vs Make Probability by Roof Type",
    subtitle = "From 1999–2024",
    x = "Distance",
    y = "Make Probability")

distance_vs_probability_roof
```

What about Surface type is there a relationship between distance and surface type when it comes to the probability of making kicks?

```{r}
distance_vs_probability_surface = ggplot(pbp_complete_binned, 
  aes(x = kick_distance, y = probability_advanced))+
  geom_point(aes(size = group_n), alpha = 0.6)+
  geom_smooth(se = FALSE) +
  geom_hline(
    yintercept = mean(pbp_complete$result, na.rm = TRUE), 
    linetype = "dashed", color = "red", alpha = 0.6)+
  facet_wrap(~ surface) +
  dark_theme_minimal() +
  labs(
    title = "NFL Kick Distance vs Make Probability by Field Surface",
    subtitle = "From 1999–2024",
    x = "Distance",
    y = "Make Probability")

distance_vs_probability_surface
```

#Early Analytical Questions

Here are some of the early questions I'm thinking about at this stage:

- in terms of general trends we can see that kicking accuracy has a negative relationship with distance (this makes sense and is our first check)

-kicking accuracy has strong(ger) negative relationships with distance in: closed, dome, and outdoor stadiums, but more of a logarithmic relationship with distance in open stadiums (research this)

-kicking accuracy on astroplay, astroturf, matrixturf, and sportturf surfaces appears to not have as drastic negative consequences over longer distances as field turf and grass (research what stadiums carry these types and if they're indoors or an even mix of different stadium types)

-dessograss and unknown surface types have almost a completely stable/level relationship with distance (this doesn't appear to be natural or make sense so more research is needed)


##Correlation Testing

Correlation between kick distance and make probability:
```{r}
distance_probability_cor = cor.test(pbp_complete_binned$kick_distance, pbp_complete_binned$probability_advanced)

distance_probability_cor
```
Strong Negative correlation (statistically significant) with distance and p(make).

Correlation between roof type and probability:
```{r}
roof_probability_cor = aov(probability_advanced ~ roof,data = pbp_complete_binned)

summary(roof_probability_cor)
```
This anova function tells us that roof type as a whole is significant in terms of its impact on p(make)

Going deeper:
```{r}
roof_probability_tukey = TukeyHSD(roof_probability_cor)

roof_probability_tukey
```
Results explained:
Kicks in dome stadiums are 0.65% more successful than closed stadiums
kicks in open stadiums are 1.24% more successful than closed stadiums
kicks in outdoor stadiums are 0.08% less successful than closed stadiums
kicks in outdoor stadiums are 1.50% less successful than dome stadiums
kicks in outdoor stadiums are 2.09% less successful than open stadiums

Conclusion ranking roof types for positive impact on kicking probabilities-
Dome and Open are the best
Closed are middle of the pack
Outdoor is significantly worse

```{r}
roof_prob_boxplot = ggplot(pbp_complete_binned, 
                             aes(x = roof, y = probability_advanced))+
                             geom_boxplot(fill = "skyblue", alpha = 0.7)+
                             dark_theme_minimal() +
                             labs(title = "Probability Advanced by Roof Type", 
                             x = "Roof Type", y = "Probability Advanced")

roof_prob_boxplot
```

Impact of surface type on kicking probability

I'm going to start with generic trends and then move onto brand types/names
```{r}
surface_type_probability_cor = aov(probability_advanced ~ surface_type,data = pbp_complete_binned)

summary(surface_type_probability_cor)
```
The type of surface used for fields is significant when it comes to making kicks

Tukey test surface types:
```{r}
surface_type_probability_tukey = TukeyHSD(surface_type_probability_cor)

surface_type_probability_tukey
```
Biggest significant takeaways-

Natural grass has ~1.85% less p(made) vs Artificial surfaces

Specific Brand/Surface Name tesing:
```{r}
surface_probability_cor = aov(probability_advanced ~ surface,data = pbp_complete_binned)

summary(surface_probability_cor)
```
Again we find that surface brands as a whole are significant in their relationship to kicking probability

Tukey test surface brands:
```{r}
surface_probability_tukey = TukeyHSD(surface_probability_cor)

surface_probability_tukey
```
Takeaways-

Natural Grass is significant worse than MatrixTurf, SportTurf, Dessograss, and Astroplay.

Dessograss, MatrixTurf, Astroplay are consistently beating other types.

FieldTurf and Astroturf have below average kicking probabilities.

Unknown brands/surface types are slightly worse than Astroplay and borderline worse than MatrixTurf, but not statistically different from Grass or SportTurf.

##Going back to probability and distance

remember in those charts earlier how I was seeing probability fit more of a logarithmic equation around distance rather than an exponential one?

Let's take a look at edge/fringe cases to see if longer kicks really do fit that pattern or what's really going on there.

```{r}
long_distance = pbp_complete_grouped%>%
  filter(kick_distance >= 50)
```

There are 3556 known attempts from 1999 onwards where a kick has been tried from 50+ yards.

What's the average make probability of those kicks as a whole:
```{r}
long_distance_average = mean(long_distance$probability_advanced)

long_distance_average
```
We can see that roughly 61.70% of kicks attempted from 50+ yards are made.

Broken down by yardage:
```{r}
long_distance_average_by_yard = long_distance%>%
  group_by(kick_distance)%>%
  summarise(average_probability = mean(probability_advanced),
            tries = n())

head(long_distance_average_by_yard, 10)
```

Plotting this out:
```{r}
long_distance_average_by_yard_plot = ggplot(long_distance_average_by_yard, 
                                            mapping = aes(x = kick_distance, 
                                                          y = average_probability))+
                                            geom_point(aes(size = tries))+
                                            geom_smooth()+
                                            dark_theme_minimal()+
                                            labs(title = "Probability of successful NFL field goal tries >= 50 yards", 
                                            subtitle = "since 1999", 
                                            x = "Yardage",
                                            y = "Probability")

long_distance_average_by_yard_plot
```

This makes much more sense so those fringe cases really aren't amounting to "better" kicking at longer distances....they're just minimal tries which should be fitting more of an exponential function over distance. 

So as a whole p(making a field goal over distance with respect to the number of tries (and not binning field goals) looks like this):
```{r}
distance_summary = pbp_complete_grouped%>%
  group_by(kick_distance)%>%
  summarise(
    average_probability = mean(probability_advanced, na.rm = TRUE),
    tries = n())


distance_vs_probability_plot_3 = ggplot(distance_summary, 
                                      aes(x = kick_distance,
                                          y = average_probability,
                                          size = tries))+
                                          geom_point(alpha = 0.6)+
                                          geom_smooth(se = FALSE)+
                                          dark_theme_minimal()+
                                          labs(title = "NFL Kick Distance vs Make Probability (Advanced)",
                                          subtitle = "from 1999–2024",
                                          x = "Distance (yards)",
                                          y = "Probability of make",
                                          size = "Attempts")

distance_vs_probability_plot_3
```

##Mixed Effect Modeling before scaling

One thing I want to do in this analysis is break down how how different players, teams, stadiums, roof types, and surfaces impact the expected kicker probabilities and make probability above expected.

Let's start off with a simple me model on kickers as a whole

```{r}
kicker_mixed_model = glmer(result ~ kick_distance + temp + roof + wind + surface + (1|kicker_player_name), 
                          data = pbp_complete, 
                          family = binomial)

summary(kicker_mixed_model)
```
Takeaways:
- std being 0.4489 shows that some kickers are better/worse than average
- +/- 0.45 log-odds translates to +/-11 percentage points in make probability
-temperature slightly increases your odds of making a kick
-wind slightly decreases your odds of making a kick
-none of the roof types or surface types are significant in terms of p(making a kick)


##Scaling

Scaling to a mean of 0 and a SD of 1
```{r}
pbp_complete$kick_distance_scaled = scale(pbp_complete$kick_distance)
pbp_complete$temp_scaled = scale(pbp_complete$temp)
pbp_complete$wind_scaled = scale(pbp_complete$wind)
```

##Mixed Effect Model with Scaling

```{r}
kicker_mixed_model = glmer(result ~ kick_distance_scaled + temp_scaled + roof + wind_scaled + surface + (1|kicker_player_name), 
                          data = pbp_complete, 
                          family = binomial, control = glmerControl(optimizer = "bobyqa"))

summary(kicker_mixed_model)
```
Takeaways after scaling:
-your odds of making a kick decrease by ~71.32% for each standard deviation (roughly 9 yards) increase from the average distance 
-odds actually increase by ~12.04% for every SD increase in temperature (~20 degrees)
-odds decrease ~10.17% for each SD increase in wind speed (~5.5mph)

##What are kicker-specific random effects?
```{r}
kicker_effects = ranef(kicker_mixed_model, condVar = TRUE)$kicker_player_name

kicker_effects = kicker_effects%>%
  arrange(desc(`(Intercept)`))

head(kicker_effects, 10)

tail(kicker_effects, 10)
```

The intercepts we see here display the impacts or effects individual kickers have on kick probability given roof type, temperature, wind speed, surface type and distance. So given all those variables guess who is head and shoulders above the rest of the league....Justin Tucker by a wide margin. Anything positive means above average and anything negative is under average

##Plotting Kicker Effects out

Lets filter down to current NFL kickers and get a gauge of who is performing above league average in terms of kick probability for all of these factors taken into consideration.


```{r}
random_effects_df = as.data.frame(kicker_effects)

random_effects_df$kicker = row.names(kicker_effects)
```


```{r}
standard_error_kicker = se.ranef(kicker_mixed_model)$kicker_player_name[,1]

random_effects_df$standard_error = standard_error_kicker

random_effects_df = random_effects_df%>%
  mutate(lower = `(Intercept)` - 1.96*standard_error,
         upper = `(Intercept)` + 1.96*standard_error)

```

Filtering down to current players
```{r}
random_effects_df = random_effects_df%>%
  filter(kicker %in% c("J.Bates", "J.Tucker", "C.Boswell", "C.Dicker", "B.Aubrey",
                       "C.McLaughlin", "J.Elliott", "T.Bass", "J.Sanders", "H.Butker",
                       "T.Loop", "K.Fairbairn", "C.Santos", "J.Karty", "M.Gay", "W.Lutz",
                       "E.McPherson", "D.Carlson", "B.McManus", "J.Myers", "W.Reichard",
                       "Y.Koo", "Z.Gonzalez", "J.Moody", "C.Little", "A.Borregales", 
                       "C.Ryland", "G.Gano", "D.Hopkins", "M.Wright", "S.Shrader", 
                       "B.Grupe", "J.Slye", "J.McAtamney", "J.Parker Romo", "C.Davis",
                       "A.Carlson", "G.Zuerlein", "N.Folk", "G.Joseph", "E.Pineiro", 
                       "A.Seibert", "M.Prater", "B.Narveson", "L.Havrisik", "C.York",
                       "T.Brown", "C.Dunn", "R.Bullock", "B.Maher", "M.Ammendola", 
                       "N.Sciba", "J.Garibay", "R.Patterson", "J. McCourt", "C.Shudak", "M.Badgley"))
```



Plot
```{r}
Kicker_Random_Effects_plot1 = ggplot(random_effects_df, aes(x = reorder(kicker, `(Intercept)`), y = `(Intercept)`))+
  geom_point()+
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2)+
  coord_flip()+
  geom_hline(yintercept = 0, linetype = "dashed")+
  dark_theme_minimal()+
  labs(title = "Current NFL Kicker-Specific Random Effects with 95% CI",
       y = "Random Intercept (Effect on Log-Odds)",
       x = "Kicker") 

Kicker_Random_Effects_plot1

```
Looking at the plot in terms of P(making a kick)
```{r}
probability_function = function(logit){
  plogis(logit)
}


random_effects_probability_df = random_effects_df%>%
  mutate(probability = probability_function(`(Intercept)`))%>%
  arrange(desc(probability))

head(random_effects_probability_df, 10)
```

Plotting
```{r}
mean_prob = mean(random_effects_probability_df$probability, na.rm = TRUE)

Kicker_Random_Effects_prob = ggplot(random_effects_probability_df, aes(x = reorder(kicker, probability), y = probability)) +
  geom_hline(yintercept = mean_prob, linetype = "dashed") +
  geom_point() +
  coord_flip() +
  dark_theme_minimal() +
  labs(title = "Current NFL Kicker-Specific Random Effects", 
       subtitle = "In terms of probability of making a kick",
       y = "Probability",
       x = "Kicker")

Kicker_Random_Effects_prob
```

Highlighting specific players

```{r}
highlighted_kicker = "W.Lutz" #Just change this to the kicker you want to study

random_effects_probability_df = random_effects_probability_df%>%
  mutate(highlight = ifelse(kicker == highlighted_kicker, "Highlighted", "Other"))

mean_prob = mean(random_effects_probability_df$probability, na.rm = TRUE)

ggplot(random_effects_probability_df, aes(x = reorder(kicker, probability), 
                                          y = probability, 
                                          fill = highlight))+
  geom_col(show.legend = FALSE)+
  geom_hline(yintercept = mean_prob, linetype = "dashed")+
  coord_flip()+
  scale_fill_manual(values = c("Highlighted" = "orange", "Other" = "gray70"))+
  dark_theme_minimal()+
  labs(title = "Kicker-Specific Random Effects", 
       subtitle = "In terms of probability of making a kick",
       y = "Probability",
       x = "Kicker")

```

##Modeling with Surface Type instead of Surface
I want to try and run another model but this time with surface (artificial or real), instead of the specific brand of surface type used to capture general trends.

```{r}
kicker_mixed_model2 = glmer(result ~ kick_distance_scaled + temp_scaled + roof + wind_scaled + surface_type + (1|kicker_player_name), 
                          data = pbp_complete, 
                          family = binomial, control = glmerControl(optimizer = "bobyqa"))

summary(kicker_mixed_model2)
```
-according to this model, natural grass has a slightly more negative impact on make probability vs artificial surfaces, but it's not statistically significant.

##Next Modeling Steps

Add game_seconds_remaining, score differential, quarter.

Add distance * wind interaction.

Consider random slope for distance per kicker.

Check AIC / model fit improvements.

Check for multicollinearity (VIF) after adding interactions.

Mutating to create a score differential variable and a score differential type variable
```{r}
pbp_complete = pbp_complete%>%
  mutate(score_differential = abs(total_home_score - total_away_score),
         score_differential_type = case_when(
           score_differential == 0 ~ "Tied",
           score_differential <= 7 ~ "One Possession",
           score_differential > 7 & score_differential <= 14 ~ "Close",
           score_differential > 14 ~ "Blowout"))

pbp_complete$score_differential_type = factor(pbp_complete$score_differential_type,
           levels = c("Tied", "One Possession", "Close", "Blowout"))

pbp_complete = pbp_complete%>%
  mutate(score_differential = as.numeric(score_differential))
  
```


Modeling with Score Differential
```{r}
kicker_mixed_model3 = glmer(result ~ kick_distance_scaled + temp_scaled + roof + wind_scaled + surface_type + score_differential +(1|kicker_player_name), 
                          data = pbp_complete, 
                          family = binomial, control = glmerControl(optimizer = "bobyqa"))

summary(kicker_mixed_model3)
```

We can see that score differential is very significant in terms of individual kickers making their kicks. In fact for each point increase in the score differential, kickers increase their odds of making an individual kick by 2.06%


Plotting this out
```{r}

prob_by_score_diff = pbp_complete%>%
  group_by(score_differential)%>%
  summarise(prob_make = mean(result))

score_differential_make_plot = ggplot(prob_by_score_diff, aes(x = score_differential, y = prob_make)) +
  geom_point() +
  geom_smooth(method = "glm", method.args = list(family = "binomial")) +
  dark_theme_minimal() +
  labs(title = "Impact of Score Differential on NFL Kick Probability",
       subtitle = "1999-2024",
       x = "Score Differential",
       y = "Probability of Make")

score_differential_make_plot
```





Quick Multi-collinearity check
```{r}
kicker_glm_fixed = glm(result ~ kick_distance_scaled + temp_scaled + roof + wind_scaled + surface_type + score_differential,
                       data = pbp_complete,
                       family = binomial)
vif(kicker_glm_fixed)
```

All of my VIF levels are under 2 which is great to see in terms of having variables in my model

Next model
```{r}
kicker_mixed_model4 = glmer(result ~ kick_distance_scaled + temp_scaled + roof + wind_scaled + surface_type + score_differential_type +(1|kicker_player_name), 
                          data = pbp_complete, 
                          family = binomial, control = glmerControl(optimizer = "bobyqa"))

summary(kicker_mixed_model4)
```
Last Mixed Effects Model
```{r}
kicker_mixed_model5 = glmer(result ~ kick_distance_scaled + 
                              temp_scaled + 
                              roof + 
                              wind_scaled + 
                              surface_type + 
                              score_differential_type + 
                              quarter_seconds_remaining +
                              half_seconds_remaining + 
                              game_seconds_remaining + 
                              (1|kicker_player_name), 
                          data = pbp_complete, 
                          family = binomial, control = glmerControl(optimizer = "bobyqa"))

summary(kicker_mixed_model5)
```

Here we can see that time remaining in a quarter, half, and game doesn't have a significant impact on a kicker's make probability.

Visualzing effects
```{r}
score_diff_effect = Effect("score_differential_type", kicker_mixed_model4)

kick_dist_effect = Effect("kick_distance_scaled", kicker_mixed_model4)

temp_effect = Effect("temp_scaled", kicker_mixed_model4)

roof_effect = Effect("roof", kicker_mixed_model4)

wind_effect = Effect("wind_scaled", kicker_mixed_model4)

surface_type_effect = Effect("surface_type", kicker_mixed_model4)

```

score differential
```{r}
plot(score_diff_effect)
```

distance
```{r}
plot(kick_dist_effect)
```

temperature
```{r}
plot(temp_effect)
```

roof types
```{r}
plot(roof_effect)
```

wind speed
```{r}
plot(wind_effect)
```

surface type
```{r}
plot(surface_type_effect)
```

## Predictive model Data

```{r}

kicker_success = pbp_complete_grouped%>%
  group_by(kicker_player_name)%>%
  summarize(kicker_fg_pct = mean(result, na.rm = TRUE))


classification_data = pbp_complete_grouped%>%
  mutate(
    score_differential = abs(total_home_score - total_away_score),
    score_differential_type = case_when(
      score_differential == 0 ~ "Tied",
      score_differential <= 7 ~ "One Possession",
      score_differential > 7 & score_differential <= 14 ~ "Close",
      score_differential > 14 ~ "Blowout"),
    posteam_type = as.factor(posteam_type),
    roof = as.factor(roof),
    surface_type = as.factor(surface_type),
    score_differential_type = as.factor(score_differential_type),
    season_type = as.factor(season_type))%>%
  left_join(kicker_success, by = "kicker_player_name")%>%
  dplyr::select(
    result, 
    kicker_fg_pct,
    kick_distance,
    wind,
    temp,
    yardline_100,
    game_seconds_remaining,
    quarter_seconds_remaining,
    posteam_type,
    roof,
    surface_type,
    score_differential_type,
    season_type)%>%
  drop_na()

```


```{r}
regression_data = pbp_complete_grouped%>%
  mutate(score_differential = abs(total_home_score - total_away_score),
         score_differential_type = case_when(
           score_differential == 0 ~ "Tied",
           score_differential <= 7 ~ "One Possession",
           score_differential > 7 & score_differential <= 14 ~ "Close",
           score_differential > 14 ~ "Blowout"),
         roof = as.factor(roof),
         score_differential_type = as.factor(score_differential_type),
         kick_distance = as.numeric(kick_distance),
         wind = as.numeric(wind),
         temp = as.numeric(temp),
         kicker_player_name = as.factor(kicker_player_name))%>%
  dplyr::select(kicker_player_name, kick_distance, wind, roof, temp, score_differential_type, probability_advanced, surface_type, yardline_100, game_seconds_remaining, quarter_seconds_remaining, goal_to_go, ydstogo, posteam_type)



```


##Building Predictive models with our Significant Variables

```{r}
#make the training data of our model everything except our target var
regression_data_model = model.matrix(probability_advanced ~ . -1, data = regression_data)

#make the probability_advanced of the kick the target var
labels = regression_data$probability_advanced
```

Train/test split
```{r}
set.seed(303)
xgbtrain_index = createDataPartition(labels, p = 0.8, list = FALSE)

xgbtrain_data = regression_data_model[xgbtrain_index, ]
xgbtrain_labels = labels[xgbtrain_index]

xgbtest_data = regression_data_model[-xgbtrain_index, ]
xgbtest_labels = labels[-xgbtrain_index]
```

Convert it to a matrix
```{r}
xgbdtrain = xgb.DMatrix(data = xgbtrain_data, label = xgbtrain_labels)
xgbdtest = xgb.DMatrix(data = xgbtest_data, label = xgbtest_labels)
```

#First XGB Predictive model: Regression
```{r}
xgb_model1 = xgboost(data = xgbdtrain, 
                    objective = "reg:squarederror", 
                    eval_metric = "rmse", 
                    nrounds = 100, 
                    verbose = 1)
```

##Predictions for regression model

```{r}
regression_predictions = predict(xgb_model1, xgbdtest)

head(regression_predictions, 5)

reg_compare_df = data.frame(Actual = xgbtest_labels,
  Predicted = regression_predictions)


reg_rmse = sqrt(mean((reg_compare_df$Actual - reg_compare_df$Predicted)^2))

reg_rmse

reg_mae = mean(abs(reg_compare_df$Actual - reg_compare_df$Predicted))

reg_mae

reg_res = sum((reg_compare_df$Actual - reg_compare_df$Predicted)^2)
reg_tot = sum((reg_compare_df$Actual - mean(reg_compare_df$Actual))^2)
reg_r_squared = 1 - (reg_res / reg_tot)

reg_r_squared
```

Plot
```{r}
reg_plot = ggplot(reg_compare_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs Actual Probability Advanced",
       x = "Actual Probability",
       y = "Predicted Probability") +
  dark_theme_minimal()

reg_plot
```

As you can see our model at trying to predict the probability a try will be successful is incredibly hard to capture especially when trying to streamline by using minimal features (limit user inputs for the app)

##Binary Classification XGBoost model

```{r}
classification_model_data = model.matrix(result ~ . -1, data = classification_data)

classification_labels = classification_data$result
```

Train-Test-Split
```{r}
set.seed(303)
class_train_index = createDataPartition(classification_labels, p = 0.8, list = FALSE)

class_train_data = classification_model_data[class_train_index, ]
class_train_labels = classification_labels[class_train_index]

class_test_data = classification_model_data[-class_train_index, ]
class_test_labels = classification_labels[-class_train_index]
```

Convert to a matrix again
```{r}
class_dtrain = xgb.DMatrix(data = class_train_data, label = class_train_labels)
class_dtest = xgb.DMatrix(data = class_test_data, label = class_test_labels)
```

Classification Model
```{r}
xgb_class_model2 = xgboost(
  data = class_dtrain,
  objective = "binary:logistic",
  eval_metric = "logloss",
  nrounds = 150,
  eta = 0.1,
  max_depth = 6,
  verbose = 1)
```

Predicting with the model
```{r}
class_predictions = predict(xgb_class_model2, class_dtest)

predicted_class = ifelse(class_predictions >= 0.5, 1, 0)
```

Confusion matrix table
```{r}
conf_matrix = table(Predicted = predicted_class, Actual = class_test_labels)

conf_matrix

confusionMatrix(
  factor(predicted_class), 
  factor(class_test_labels), 
  positive = "1")
```

AUC Analysis
```{r}
roc_obj = roc(class_test_labels, class_predictions)

auc_val = auc(roc_obj)

plot(roc_obj, col = "orange", main = "ROC Curve - Kick Make Probability Model")
```

Thoughts:
at 57% balanced accuracy and 99% sensitivity, a team could potentially deploy this model with an application to give them a sense of whether or no their kicker would make the attempt.....but I want to re-calibrate it so it learns better....you can tell the distribution of results is heavily skewed towards makes (90% of the results are 1's). Another trick here is entering in these variables as inputs before the try:

- kicker_fg_pct taken as the their mean historical make probability
- kick_distance easily obtainable
- wind default of 0 indoors and also should be readily available via weather data
- temp default of 70 indoors and also readily available via a weather app
- yardline_100 easily obtainable
- game_seconds_remaining converting to seconds here might be the trickiest part
- quarter_seconds_remaining again slightly tricky
- posteam_type is the kicker home or away
- roof what is the stadium type?
- surface_type artificial or natural grass?
- score_differential_type I've created my factor examples to reference
- season_type regular or postseason?

##Balancing out the data

Balance with Rose package
```{r}
balanced_data = ovun.sample(result ~ ., data = classification_data, method = "both", p=0.5, seed = 303, N = 2 * sum(classification_data$result == 0))$data

balanced_model_data = model.matrix(result ~ . -1, data = balanced_data)

balanced_labels = balanced_data$result
```

Train-Test-Split
```{r}
set.seed(303)

balanced_train_index = createDataPartition(balanced_labels, p = 0.8, list = FALSE)

balanced_train_data = balanced_model_data[balanced_train_index, ]

balanced_train_labels = balanced_labels[balanced_train_index]

balanced_test_data = balanced_model_data[-balanced_train_index, ]

balanced_test_labels = balanced_labels[-balanced_train_index]
```

Convert to a matrix again
```{r}
balanced_dtrain = xgb.DMatrix(data = balanced_train_data, label = balanced_train_labels)

balanced_dtest = xgb.DMatrix(data = balanced_test_data, label = balanced_test_labels)
```

Classification Model
```{r}
xgb_balanced_model1 = xgboost(
  data = balanced_dtrain,
  objective = "binary:logistic",
  eval_metric = "logloss",
  nrounds = 150,
  eta = 0.1,
  max_depth = 6,
  verbose = 1)
```

Predict with Balanced Data
```{r}
balanced_predictions = predict(xgb_balanced_model1, balanced_dtest)

predicted_balanced_class = ifelse(balanced_predictions >= 0.5, 1, 0)

confusionMatrix(
  factor(predicted_balanced_class), 
  factor(balanced_test_labels), 
  positive = "1")
```


Balanced Model AUC Analysis
```{r}
roc_obj2 = roc(balanced_test_labels, balanced_predictions)


plot(roc_obj2, col = "#1c61b6", lwd = 2, main = "ROC Curve - Balanced XGBoost Model")
abline(a = 0, b = 1, lty = 2, col = "gray")
```

##Tuning and Threshold Goals 

If I'm an analyst advising my kicking staff, I want to maximize Precision. I would also want to simultaneously want to minimize false positives in terms of telling them we are going to make the field goal/xp attempt when we might not....

Create a function to go through a stepped sequence of thresholds and calculate which gives us the best accuracy scores.
```{r}
thresholds = seq(0, 1, by = 0.01)

pr_metrics = sapply(thresholds, function(t) {
  preds = ifelse(balanced_predictions >= t, 1, 0)
  
  preds_factor = factor(preds, levels = c(0, 1))
  
  actuals_factor = factor(balanced_test_labels, levels = c(0, 1))
  
  cm = table(Predicted = preds_factor, Actual = actuals_factor)
  
  TP = cm["1", "1"]
  
  FP = cm["1", "0"]
  
  TN = cm["0", "0"]
  
  FN = cm["0", "1"]
  
  
  Precision = ifelse((TP + FP) == 0, NA, TP / (TP + FP))
  
  Recall = ifelse((TP + FN) == 0, NA, TP / (TP + FN))
  
  Specificity = ifelse((TN + FP) == 0, NA, TN / (TN + FP))
  
  F1 = ifelse(is.na(Precision) | is.na(Recall) | (Precision + Recall) == 0, NA, 2 * Precision * Recall / (Precision + Recall))
  
  Balanced_Accuracy = ifelse(is.na(Recall) | is.na(Specificity), NA, (Recall + Specificity) / 2)
  
  c(Threshold = t, Precision = Precision, Recall = Recall, Specificity = Specificity, F1 = F1, Balanced_Accuracy = Balanced_Accuracy)
})

pr_metrics = as.data.frame(t(pr_metrics))


```


Maximize Precision
```{r}
best_precision_threshold = pr_metrics$Threshold[which.max(pr_metrics$Precision)]

best_f1_threshold = pr_metrics$Threshold[which.max(pr_metrics$F1)]

best_bal_acc_threshold = pr_metrics$Threshold[which.max(pr_metrics$Balanced_Accuracy)]

best_precision_threshold

best_f1_threshold

best_bal_acc_threshold
```
If we really want to emphasize using a model ONLY when we know a kick is going to be made, the threshold for classifying a kick as successful is .99 (out of 1). If we want to prioritize hitting on the most true positives while also minimizing false positives, we should set it as .39 Lastly, if we want the most balanced model in terms of overall accuracy, it should stay just over .5 at .52.


##What does each of these look like in our base model in terms of deployment
Precision
```{r}
final_pred_prec = ifelse(balanced_predictions >= best_precision_threshold, 1, 0)

confusionMatrix(
  factor(final_pred_prec), 
  factor(balanced_test_labels), 
  positive = "1")
```

The harshest model by far.
f1
```{r}
final_pred_f1 = ifelse(balanced_predictions >= best_f1_threshold, 1, 0)

confusionMatrix(
  factor(final_pred_f1), 
  factor(balanced_test_labels), 
  positive = "1")
```

Not bad. Pretty balanced all around

balanced accuracy
```{r}
final_pred_bal = ifelse(balanced_predictions >= best_bal_acc_threshold, 1, 0)

confusionMatrix(
  factor(final_pred_bal), 
  factor(balanced_test_labels), 
  positive = "1")
```

ever so slightly better than the f1 model.

##Tuning the model and using the balanced accuracy threshold:

start with how many folds we'll go through
```{r}
cv_folds = createFolds(balanced_train_labels, k = 5, list = TRUE, returnTrain = TRUE)
```

Creating a grid of all the parameters we will tune
```{r}
param_grid = expand.grid(
  eta = c(0.01, 0.05, 0.1),
  max_depth = c(4, 6, 8),
  min_child_weight = c(1, 5, 10),
  subsample = c(0.8, 1),
  colsample_bytree = c(0.8, 1),
  gamma = c(0, 1))
```

Loop through all of the parameters, test them in combination with the balanced model and give me the accuracy scores for each iteration
```{r}
results = list()

for(i in 1:nrow(param_grid)) {
  
  params = list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = param_grid$eta[i],
    max_depth = param_grid$max_depth[i],
    min_child_weight = param_grid$min_child_weight[i],
    subsample = param_grid$subsample[i],
    colsample_bytree = param_grid$colsample_bytree[i],
    gamma = param_grid$gamma[i])
  
  cv_model = xgb.cv(
    params = params,
    data = balanced_dtrain,
    nrounds = 150,
    folds = cv_folds,
    prediction = TRUE,
    early_stopping_rounds = 10,
    maximize = FALSE,
    verbose = 0)
  
  preds = cv_model$pred
  
  actuals = balanced_train_labels
  

  threshold = 0.52
  
  predicted_class = ifelse(preds >= threshold, 1, 0)
  
  cm = table(factor(predicted_class, levels = c(0,1)), factor(actuals, levels = c(0,1)))
  
  TP = cm["1", "1"]
  
  TN = cm["0", "0"]
  
  FP = cm["1", "0"]
  
  FN = cm["0", "1"]
  
  Sensitivity = TP / (TP + FN)
  
  Specificity = TN / (TN + FP)
  
  Balanced_Accuracy = (Sensitivity + Specificity) / 2
  
  results[[i]] = c(param_grid[i, ], Balanced_Accuracy = Balanced_Accuracy)
  
  cat("Tested params set:", i, " | Balanced Accuracy:", round(Balanced_Accuracy, 4), "\n")
}
```

Best Parameters to use in the final xgboost model
```{r}
bal_results_df = bind_rows(results)

best_params = bal_results_df[which.max(bal_results_df$Balanced_Accuracy), ]

best_params
```

Taking the best parameters and putting them in a final version of my model:
```{r}
xgb_balanced_model_final = xgboost(
  data = balanced_dtrain,
  objective = "binary:logistic",
  eval_metric = "logloss",
  nrounds = 150,
  eta = 0.1,
  max_depth = 8,
  min_child_weight = 1,
  subsample = 0.8,
  colsubsample = 1,
  gamma = 0,
  verbose = 1)
```

Predicting with final model
```{r}
balanced_predictions_final = predict(xgb_balanced_model_final, balanced_dtest)

predicted_balanced_class_final = ifelse(balanced_predictions_final >= 0.52, 1, 0)

confusionMatrix(
  factor(predicted_balanced_class_final), 
  factor(balanced_test_labels), 
  positive = "1")
```
Final Model AUC Analysis
```{r}
roc_obj_final = roc(balanced_test_labels, balanced_predictions_final)


plot(roc_obj_final, col = "forestgreen", lwd = 2, main = "ROC Curve - Balanced XGBoost Model")
abline(a = 0, b = 1, lty = 2, col = "gray")
```

##Making this final model an app that Special Teams coaching staff/analysts can deploy

First I want to create a cheat sheet for users to have since instead of using kicker names in the model, we used their mean fg success rate.

save pbp_complete_grouped and final model
```{r}
write.csv(pbp_complete_grouped, "G:/My Drive/Personal Projects/pbp_complete_grouped.csv", row.names = FALSE)
```
xgb model
```{r}
xgb.save(xgb_balanced_model_final, "G:/My Drive/Personal Projects/nfl_kicking_balanced_model_final.model")
```

##Answering the Question:How do we rank kickers in the NFL Dynamically and who are the best kickers in the league right now? (For both Field Goals and XP's)

creating makes above expected variable based on how the kicker historically performed and what the probability of each kick was statistically
```{r}
ranked_kickers = pbp_complete_grouped%>%
  mutate(makes_above_expected = result - probability_advanced)
```

Ranking kickers
```{r}
ranked_kickers_total = ranked_kickers%>%
  group_by(kicker_player_name)%>%
  summarise(kicks_attempted = n(),
            kicks_made = sum(result),
            percentage = (kicks_made / kicks_attempted),
            total_above_expected = sum(makes_above_expected),
            average_above_expected = mean(makes_above_expected),
            .groups = 'drop')%>%
  arrange(desc(total_above_expected))

head(ranked_kickers_total, 10)
```

Ranked kickers by average above expected
```{r}
ranked_kickers_average = ranked_kickers%>%
  group_by(kicker_player_name)%>%
  summarise(kicks_attempted = n(),
            kicks_made = sum(result),
            percentage = (kicks_made / kicks_attempted),
            total_above_expected = sum(makes_above_expected),
            average_above_expected = mean(makes_above_expected),
            .groups = 'drop')%>%
  arrange(desc(average_above_expected))

head(ranked_kickers_average, 10)

```

Historically these are our best performers on kicking performances above expected probabilities of each kick being made

##Looking at current players only:
```{r}
ranked_kickers_total_current = ranked_kickers%>%
  filter(kicker_player_name %in% c("J.Bates", "J.Tucker", "C.Boswell", "C.Dicker", "B.Aubrey", "C.McLaughlin", "J.Elliott", "T.Bass", "J.Sanders", "H.Butker", "T.Loop", "K.Fairbairn", "C.Santos", "J.Karty", "M.Gay", "W.Lutz", "E.McPherson", "D.Carlson", "B.McManus", "J.Myers", "W.Reichard", "Y.Koo", "Z.Gonzalez", "J.Moody", "C.Little", "A.Borregales", "C.Ryland", "G.Gano", "D.Hopkins", "M.Wright", "S.Shrader", "B.Grupe", "J.Slye", "J.McAtamney", "J.Parker Romo", "C.Davis", "A.Carlson", "G.Zuerlein", "N.Folk", "G.Joseph", "E.Pineiro", "A.Seibert", "M.Prater", "B.Narveson", "L.Havrisik", "C.York", "T.Brown", "C.Dunn", "R.Bullock", "B.Maher", "M.Ammendola", "N.Sciba", "J.Garibay", "R.Patterson", "J. McCourt", "C.Shudak", "M.Badgley"))%>%
  group_by(kicker_player_name)%>%
  summarise(kicks_attempted = n(),
            kicks_made = sum(result),
            percentage = (kicks_made / kicks_attempted),
            total_above_expected = sum(makes_above_expected),
            average_above_expected = mean(makes_above_expected),
            .groups = 'drop')%>%
  arrange(desc(total_above_expected))

head(ranked_kickers_total_current, 10)
```

current players but for percentage above expected
```{r}
ranked_kickers_average_current = ranked_kickers%>%
  filter(kicker_player_name %in% c("J.Bates", "J.Tucker", "C.Boswell", "C.Dicker", "B.Aubrey", "C.McLaughlin", "J.Elliott", "T.Bass", "J.Sanders", "H.Butker", "T.Loop", "K.Fairbairn", "C.Santos", "J.Karty", "M.Gay", "W.Lutz", "E.McPherson", "D.Carlson", "B.McManus", "J.Myers", "W.Reichard", "Y.Koo", "Z.Gonzalez", "J.Moody", "C.Little", "A.Borregales", "C.Ryland", "G.Gano", "D.Hopkins", "M.Wright", "S.Shrader", "B.Grupe", "J.Slye", "J.McAtamney", "J.Parker Romo", "C.Davis", "A.Carlson", "G.Zuerlein", "N.Folk", "G.Joseph", "E.Pineiro", "A.Seibert", "M.Prater", "B.Narveson", "L.Havrisik", "C.York", "T.Brown", "C.Dunn", "R.Bullock", "B.Maher", "M.Ammendola", "N.Sciba", "J.Garibay", "R.Patterson", "J. McCourt", "C.Shudak", "M.Badgley"))%>%
  group_by(kicker_player_name)%>%
  summarise(kicks_attempted = n(),
            kicks_made = sum(result),
            percentage = (kicks_made / kicks_attempted),
            total_above_expected = sum(makes_above_expected),
            average_above_expected = mean(makes_above_expected),
            .groups = 'drop')%>%
  arrange(desc(average_above_expected))

head(ranked_kickers_average_current, 10)
```











