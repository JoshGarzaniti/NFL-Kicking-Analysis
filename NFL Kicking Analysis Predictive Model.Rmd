---
title: "NFL Kicking Analysis Predictive Model"
author: "Josh Garzaniti"
date: "2025-08-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

libraries:
```{r}
library(ggplot2)
library(ggdark)
library(caret)
library(tidyverse)
library(tidyr)
library(stringr)
library(xgboost)
library(randomForest)
library(ggrepel)
library(nflreadr)
library(nflplotR)
library(nflfastR)
library(mice)
library(lme4)
library(arm)
library(car)
library(effects)
library(Matrix)
library(pROC)
library(SHAPforxgboost)
library(ROSE)
library(Metrics)
library(shiny)
library(DT)
```

load in pbp_complete data
```{r}
pbp_complete_grouped = read.csv("G:/My Drive/Personal Projects/pbp_complete_grouped.csv")
```

## Predictive model Data

```{r}

kicker_success = pbp_complete_grouped%>%
  group_by(kicker_player_name)%>%
  summarize(kicker_fg_pct = mean(result, na.rm = TRUE))


classification_data = pbp_complete_grouped%>%
  mutate(
    score_differential = abs(total_home_score - total_away_score),
    score_differential_type = case_when(
      score_differential == 0 ~ "Tied",
      score_differential <= 7 ~ "One Possession",
      score_differential > 7 & score_differential <= 14 ~ "Close",
      score_differential > 14 ~ "Blowout"),
    posteam_type = as.factor(posteam_type),
    roof = as.factor(roof),
    surface_type = as.factor(surface_type),
    score_differential_type = as.factor(score_differential_type),
    season_type = as.factor(season_type))%>%
  left_join(kicker_success, by = "kicker_player_name")%>%
  dplyr::select(
    result, 
    kicker_fg_pct,
    kick_distance,
    wind,
    temp,
    yardline_100,
    game_seconds_remaining,
    quarter_seconds_remaining,
    posteam_type,
    roof,
    surface_type,
    score_differential_type,
    season_type)%>%
  drop_na()

```


```{r}
regression_data = pbp_complete_grouped%>%
  mutate(score_differential = abs(total_home_score - total_away_score),
         score_differential_type = case_when(
           score_differential == 0 ~ "Tied",
           score_differential <= 7 ~ "One Possession",
           score_differential > 7 & score_differential <= 14 ~ "Close",
           score_differential > 14 ~ "Blowout"),
         roof = as.factor(roof),
         score_differential_type = as.factor(score_differential_type),
         kick_distance = as.numeric(kick_distance),
         wind = as.numeric(wind),
         temp = as.numeric(temp),
         kicker_player_name = as.factor(kicker_player_name))%>%
  dplyr::select(kicker_player_name, kick_distance, wind, roof, temp, score_differential_type, probability_advanced, surface_type, yardline_100, game_seconds_remaining, quarter_seconds_remaining, goal_to_go, ydstogo, posteam_type)



```


##Building Predictive models with our Significant Variables

```{r}
#make the training data of our model everything except our target var
regression_data_model = model.matrix(probability_advanced ~ . -1, data = regression_data)

#make the probability_advanced of the kick the target var
labels = regression_data$probability_advanced
```

Train/test split
```{r}
set.seed(303)
xgbtrain_index = createDataPartition(labels, p = 0.8, list = FALSE)

xgbtrain_data = regression_data_model[xgbtrain_index, ]
xgbtrain_labels = labels[xgbtrain_index]

xgbtest_data = regression_data_model[-xgbtrain_index, ]
xgbtest_labels = labels[-xgbtrain_index]
```

Convert it to a matrix
```{r}
xgbdtrain = xgb.DMatrix(data = xgbtrain_data, label = xgbtrain_labels)
xgbdtest = xgb.DMatrix(data = xgbtest_data, label = xgbtest_labels)
```

#First XGB Predictive model: Regression
```{r}
xgb_model1 = xgboost(data = xgbdtrain, 
                    objective = "reg:squarederror", 
                    eval_metric = "rmse", 
                    nrounds = 100, 
                    verbose = 1)
```

##Predictions for regression model

```{r}
regression_predictions = predict(xgb_model1, xgbdtest)

head(regression_predictions, 5)

reg_compare_df = data.frame(Actual = xgbtest_labels,
  Predicted = regression_predictions)


reg_rmse = sqrt(mean((reg_compare_df$Actual - reg_compare_df$Predicted)^2))

reg_rmse

reg_mae = mean(abs(reg_compare_df$Actual - reg_compare_df$Predicted))

reg_mae

reg_res = sum((reg_compare_df$Actual - reg_compare_df$Predicted)^2)
reg_tot = sum((reg_compare_df$Actual - mean(reg_compare_df$Actual))^2)
reg_r_squared = 1 - (reg_res / reg_tot)

reg_r_squared
```

Plot
```{r}
reg_plot = ggplot(reg_compare_df, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Predicted vs Actual Probability Advanced",
       x = "Actual Probability",
       y = "Predicted Probability") +
  dark_theme_minimal()

reg_plot
```

As you can see our model at trying to predict the probability a try will be successful is incredibly hard to capture especially when trying to streamline by using minimal features (limit user inputs for the app)

##Binary Classification XGBoost model

```{r}
classification_model_data = model.matrix(result ~ . -1, data = classification_data)

classification_labels = classification_data$result
```

Train-Test-Split
```{r}
set.seed(303)
class_train_index = createDataPartition(classification_labels, p = 0.8, list = FALSE)

class_train_data = classification_model_data[class_train_index, ]
class_train_labels = classification_labels[class_train_index]

class_test_data = classification_model_data[-class_train_index, ]
class_test_labels = classification_labels[-class_train_index]
```

Convert to a matrix again
```{r}
class_dtrain = xgb.DMatrix(data = class_train_data, label = class_train_labels)
class_dtest = xgb.DMatrix(data = class_test_data, label = class_test_labels)
```

Classification Model
```{r}
xgb_class_model2 = xgboost(
  data = class_dtrain,
  objective = "binary:logistic",
  eval_metric = "logloss",
  nrounds = 150,
  eta = 0.1,
  max_depth = 6,
  verbose = 1)
```

Predicting with the model
```{r}
class_predictions = predict(xgb_class_model2, class_dtest)

predicted_class = ifelse(class_predictions >= 0.5, 1, 0)
```

Confusion matrix table
```{r}
conf_matrix = table(Predicted = predicted_class, Actual = class_test_labels)

conf_matrix

confusionMatrix(
  factor(predicted_class), 
  factor(class_test_labels), 
  positive = "1")
```

AUC Analysis
```{r}
roc_obj = roc(class_test_labels, class_predictions)

auc_val = auc(roc_obj)

plot(roc_obj, col = "orange", main = "ROC Curve - Kick Make Probability Model")
```

Thoughts:
at 57% balanced accuracy and 99% sensitivity, a team could potentially deploy this model with an application to give them a sense of whether or no their kicker would make the attempt.....but I want to re-calibrate it so it learns better....you can tell the distribution of results is heavily skewed towards makes (90% of the results are 1's). Another trick here is entering in these variables as inputs before the try:

- kicker_fg_pct taken as the their mean historical make probability
- kick_distance easily obtainable
- wind default of 0 indoors and also should be readily available via weather data
- temp default of 70 indoors and also readily available via a weather app
- yardline_100 easily obtainable
- game_seconds_remaining converting to seconds here might be the trickiest part
- quarter_seconds_remaining again slightly tricky
- posteam_type is the kicker home or away
- roof what is the stadium type?
- surface_type artificial or natural grass?
- score_differential_type I've created my factor examples to reference
- season_type regular or postseason?

##Balancing out the data

Balance with Rose package
```{r}
balanced_data = ovun.sample(result ~ ., data = classification_data, method = "both", p=0.5, seed = 303, N = 2 * sum(classification_data$result == 0))$data

balanced_model_data = model.matrix(result ~ . -1, data = balanced_data)

balanced_labels = balanced_data$result
```

Train-Test-Split
```{r}
set.seed(303)

balanced_train_index = createDataPartition(balanced_labels, p = 0.8, list = FALSE)

balanced_train_data = balanced_model_data[balanced_train_index, ]

balanced_train_labels = balanced_labels[balanced_train_index]

balanced_test_data = balanced_model_data[-balanced_train_index, ]

balanced_test_labels = balanced_labels[-balanced_train_index]
```

Convert to a matrix again
```{r}
balanced_dtrain = xgb.DMatrix(data = balanced_train_data, label = balanced_train_labels)

balanced_dtest = xgb.DMatrix(data = balanced_test_data, label = balanced_test_labels)
```

Classification Model
```{r}
xgb_balanced_model1 = xgboost(
  data = balanced_dtrain,
  objective = "binary:logistic",
  eval_metric = "logloss",
  nrounds = 150,
  eta = 0.1,
  max_depth = 6,
  verbose = 1)
```

Predict with Balanced Data
```{r}
balanced_predictions = predict(xgb_balanced_model1, balanced_dtest)

predicted_balanced_class = ifelse(balanced_predictions >= 0.5, 1, 0)

confusionMatrix(
  factor(predicted_balanced_class), 
  factor(balanced_test_labels), 
  positive = "1")
```


Balanced Model AUC Analysis
```{r}
roc_obj2 = roc(balanced_test_labels, balanced_predictions)


plot(roc_obj2, col = "#1c61b6", lwd = 2, main = "ROC Curve - Balanced XGBoost Model")
abline(a = 0, b = 1, lty = 2, col = "gray")
```

##Tuning and Threshold Goals 

If I'm an analyst advising my kicking staff, I want to maximize Precision. I would also want to simultaneously want to minimize false positives in terms of telling them we are going to make the field goal/xp attempt when we might not....

Create a function to go through a stepped sequence of thresholds and calculate which gives us the best accuracy scores.
```{r}
thresholds = seq(0, 1, by = 0.01)

pr_metrics = sapply(thresholds, function(t) {
  preds = ifelse(balanced_predictions >= t, 1, 0)
  
  preds_factor = factor(preds, levels = c(0, 1))
  
  actuals_factor = factor(balanced_test_labels, levels = c(0, 1))
  
  cm = table(Predicted = preds_factor, Actual = actuals_factor)
  
  TP = cm["1", "1"]
  
  FP = cm["1", "0"]
  
  TN = cm["0", "0"]
  
  FN = cm["0", "1"]
  
  
  Precision = ifelse((TP + FP) == 0, NA, TP / (TP + FP))
  
  Recall = ifelse((TP + FN) == 0, NA, TP / (TP + FN))
  
  Specificity = ifelse((TN + FP) == 0, NA, TN / (TN + FP))
  
  F1 = ifelse(is.na(Precision) | is.na(Recall) | (Precision + Recall) == 0, NA, 2 * Precision * Recall / (Precision + Recall))
  
  Balanced_Accuracy = ifelse(is.na(Recall) | is.na(Specificity), NA, (Recall + Specificity) / 2)
  
  c(Threshold = t, Precision = Precision, Recall = Recall, Specificity = Specificity, F1 = F1, Balanced_Accuracy = Balanced_Accuracy)
})

pr_metrics = as.data.frame(t(pr_metrics))


```


Maximize Precision
```{r}
best_precision_threshold = pr_metrics$Threshold[which.max(pr_metrics$Precision)]

best_f1_threshold = pr_metrics$Threshold[which.max(pr_metrics$F1)]

best_bal_acc_threshold = pr_metrics$Threshold[which.max(pr_metrics$Balanced_Accuracy)]

best_precision_threshold

best_f1_threshold

best_bal_acc_threshold
```
If we really want to emphasize using a model ONLY when we know a kick is going to be made, the threshold for classifying a kick as successful is .99 (out of 1). If we want to prioritize hitting on the most true positives while also minimizing false positives, we should set it as .39 Lastly, if we want the most balanced model in terms of overall accuracy, it should stay just over .5 at .52.


##What does each of these look like in our base model in terms of deployment
Precision
```{r}
final_pred_prec = ifelse(balanced_predictions >= best_precision_threshold, 1, 0)

confusionMatrix(
  factor(final_pred_prec), 
  factor(balanced_test_labels), 
  positive = "1")
```

The harshest model by far.
f1
```{r}
final_pred_f1 = ifelse(balanced_predictions >= best_f1_threshold, 1, 0)

confusionMatrix(
  factor(final_pred_f1), 
  factor(balanced_test_labels), 
  positive = "1")
```

Not bad. Pretty balanced all around

balanced accuracy
```{r}
final_pred_bal = ifelse(balanced_predictions >= best_bal_acc_threshold, 1, 0)

confusionMatrix(
  factor(final_pred_bal), 
  factor(balanced_test_labels), 
  positive = "1")
```

ever so slightly better than the f1 model.

##Tuning the model and using the balanced accuracy threshold:

start with how many folds we'll go through
```{r}
cv_folds = createFolds(balanced_train_labels, k = 5, list = TRUE, returnTrain = TRUE)
```

Creating a grid of all the parameters we will tune
```{r}
param_grid = expand.grid(
  eta = c(0.01, 0.05, 0.1),
  max_depth = c(4, 6, 8),
  min_child_weight = c(1, 5, 10),
  subsample = c(0.8, 1),
  colsample_bytree = c(0.8, 1),
  gamma = c(0, 1))
```

Loop through all of the parameters, test them in combination with the balanced model and give me the accuracy scores for each iteration
```{r}
results = list()

for(i in 1:nrow(param_grid)) {
  
  params = list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = param_grid$eta[i],
    max_depth = param_grid$max_depth[i],
    min_child_weight = param_grid$min_child_weight[i],
    subsample = param_grid$subsample[i],
    colsample_bytree = param_grid$colsample_bytree[i],
    gamma = param_grid$gamma[i])
  
  cv_model = xgb.cv(
    params = params,
    data = balanced_dtrain,
    nrounds = 150,
    folds = cv_folds,
    prediction = TRUE,
    early_stopping_rounds = 10,
    maximize = FALSE,
    verbose = 0)
  
  preds = cv_model$pred
  
  actuals = balanced_train_labels
  

  threshold = 0.52
  
  predicted_class = ifelse(preds >= threshold, 1, 0)
  
  cm = table(factor(predicted_class, levels = c(0,1)), factor(actuals, levels = c(0,1)))
  
  TP = cm["1", "1"]
  
  TN = cm["0", "0"]
  
  FP = cm["1", "0"]
  
  FN = cm["0", "1"]
  
  Sensitivity = TP / (TP + FN)
  
  Specificity = TN / (TN + FP)
  
  Balanced_Accuracy = (Sensitivity + Specificity) / 2
  
  results[[i]] = c(param_grid[i, ], Balanced_Accuracy = Balanced_Accuracy)
  
  cat("Tested params set:", i, " | Balanced Accuracy:", round(Balanced_Accuracy, 4), "\n")
}
```

Best Parameters to use in the final xgboost model
```{r}
bal_results_df = bind_rows(results)

best_params = bal_results_df[which.max(bal_results_df$Balanced_Accuracy), ]

best_params
```

Taking the best parameters and putting them in a final version of my model:
```{r}
xgb_balanced_model_final = xgboost(
  data = balanced_dtrain,
  objective = "binary:logistic",
  eval_metric = "logloss",
  nrounds = 150,
  eta = 0.1,
  max_depth = 8,
  min_child_weight = 1,
  subsample = 0.8,
  colsubsample = 1,
  gamma = 0,
  verbose = 1)
```

Predicting with final model
```{r}
balanced_predictions_final = predict(xgb_balanced_model_final, balanced_dtest)

predicted_balanced_class_final = ifelse(balanced_predictions_final >= 0.52, 1, 0)

confusionMatrix(
  factor(predicted_balanced_class_final), 
  factor(balanced_test_labels), 
  positive = "1")
```

Final Model AUC Analysis
```{r}
roc_obj_final = roc(balanced_test_labels, balanced_predictions_final)


plot(roc_obj_final, col = "forestgreen", lwd = 2, main = "ROC Curve - Balanced XGBoost Model")
abline(a = 0, b = 1, lty = 2, col = "gray")
```

##Making this final model an app that Special Teams coaching staff/analysts can deploy

First I want to create a cheat sheet for users to have since instead of using kicker names in the model, we used their mean fg success rate.

save pbp_complete_grouped df
```{r}
write.csv(pbp_complete_grouped, "G:/My Drive/Personal Projects/pbp_complete_grouped.csv", row.names = FALSE)
```

save our xgb model
```{r}
xgb.save(xgb_balanced_model_final, "G:/My Drive/Personal Projects/nfl_kicking_balanced_model_final.model")
```